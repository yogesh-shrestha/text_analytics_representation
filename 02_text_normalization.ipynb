{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1a7d15",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "Text normalization is defined as a process that consists of a series of steps that should be followed to clean and standardize textual data into a form that could be consumed by other NLP and analytics systems and applications as input. Tokenization itself is a part of text normalization. Besides tokenization, various other techniques include cleaning text, case conversion, correcting spellings, removing stopwords and other unncessary terms, stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c2a9c",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Usually, we tokenize text before or after removing unnecessary characters and symbols\n",
    "from the data. This choice depends on the problem we are trying to solve and the data\n",
    "we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d59e62",
   "metadata": {},
   "source": [
    "### Removing special Characters\n",
    "One important task in text normalization involves removing unnecessary and special\n",
    "characters. These may be special symbols or even punctuation that occurs in sentences.\n",
    "This step is often performed before or after tokenization. The main reason for doing so is\n",
    "because often punctuation or special characters do not have much significance when we\n",
    "analyze the text and utilize it for extracting features or information based on NLP and ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fcc81e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hours', 'later', 'rumors', 'began', 'to', 'spread', 'in', 'the', 'closeknit', 'community', 'that', 'something', 'had', 'happened', 'to', 'her', 'son', 'in', 'the', 'nearby', 'river', 'Kapessa', '’', 's', 'older', 'sister', 'heard', 'from', 'friends', 'that', 'Kapessa', 'who', 'could', 'not', 'swim', 'may', 'have', 'jumped', 'into', 'the', 'water', 'others', 'alleged', 'that', 'he', 'had', 'been', 'pushed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def remove_special_chars(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    new_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return new_tokens\n",
    "\n",
    "sample_text = \"\"\"Hours later, rumors began to spread in the close-knit community that something \\\n",
    "had happened to her son in the nearby river. Kapessa’s older sister heard from friends that \\\n",
    "Kapessa, who could not swim, may have jumped into the water; others alleged that he had been \\\n",
    "pushed.\"\"\"\n",
    "words = nltk.word_tokenize(sample_text)\n",
    "\n",
    "new_words = remove_special_chars(words)\n",
    "print(list(new_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677bc08",
   "metadata": {},
   "source": [
    "### Removing Accented Characters\n",
    "We might be dealing with accented characters/letters, especially\n",
    "if you only want to analyze the English language. Hence, we need to make sure that these\n",
    "characters are converted and standardized into ASCII characters. This shows a simple\n",
    "example — converting é to e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174c08ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "str1 = 'Sómě Áccěntěd těxt'\n",
    "remove_accented_chars(str1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ecef1",
   "metadata": {},
   "source": [
    "### Expanding Contractions\n",
    "Contractions are shortened version of words or syllables. They exist in either written or\n",
    "spoken forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7215b929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you all cannot expand contractions I would think'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "contractions.fix(\"Y'all can't expand contractions I'd think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7c6ce",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Stopwords are words that have little or no significance. They are usually removed from text during processing so as to retain words having maximum significance and context. Stopwords are usually words that end up occurring the most if you aggregated any corpus of text based on singular tokens and checked their frequencies. Words like a, the , me , and so on are stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5090790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hours', 'later', ',', 'rumors', 'began', 'spread', 'close-knit', 'community', 'something', 'happened', 'son', 'nearby', 'river', '.', 'Kapessa', '’', 'older', 'sister', 'heard', 'friends', 'Kapessa', ',', 'could', 'swim', ',', 'may', 'jumped', 'water', ';', 'others', 'alleged', 'pushed', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "sample_text = \"\"\"Hours later, rumors began to spread in the close-knit community that something \\\n",
    "had happened to her son in the nearby river. Kapessa’s older sister heard from friends that \\\n",
    "Kapessa, who could not swim, may have jumped into the water; others alleged that he had been \\\n",
    "pushed.\"\"\"\n",
    "\n",
    "print(remove_stopwords(nltk.word_tokenize(sample_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c76da9",
   "metadata": {},
   "source": [
    "### Correcting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ad9e859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "\n",
    "incorrect_word = textblob.Word('fianly')\n",
    "incorrect_word.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36da11",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "A morpheme is the smallest meaningful lexical item in a language. A morpheme is not necessarily the same as a word. The main difference between a morpheme and a word is that a morpheme sometimes does not stand alone, but a word, by definition, always stands alone. Word stems are also often known as the <i>base form</i> of a word, and we can create new words by attaching affixes to them in a process known as <i>inflection</i> . The reverse of this is obtaining the base form of a word from its inflected form, and this is known as <i>stemming</i> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06dae67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lie\n",
      "strang\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('jumping'), stemmer.stem('jumps'), stemmer.stem('jumped'))\n",
    "print(stemmer.stem('lying'))\n",
    "print(stemmer.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba18ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lying\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('jumping'), stemmer.stem('jumps'), stemmer.stem('jumped'))\n",
    "print(stemmer.stem('lying'))\n",
    "print(stemmer.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4c6ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lie\n",
      "strang\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print(stemmer.stem('jumping'), stemmer.stem('jumps'), stemmer.stem('jumped'))\n",
    "print(stemmer.stem('lying'))\n",
    "print(stemmer.stem('strange'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2db26f",
   "metadata": {},
   "source": [
    "### Lemmatization \n",
    "The process of <i>lemmatization</i> is very similar to stemming—you remove word affixes to\n",
    "get to a base form of the word. But in this case, this base form is also known as the <i>root\n",
    "word</i> , but not the <i>root stem</i> . The difference is that the root stem may not always be a lexicographically correct word; that is, it may not be present in the dictionary. The root\n",
    "word, also known as the <i>lemma</i> , will always be present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e8b0982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n",
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize nouns\n",
    "print(lemmatizer.lemmatize('cars', 'n'))\n",
    "print(lemmatizer.lemmatize('men', 'n'))\n",
    "\n",
    "# lemmatize verbs\n",
    "print(lemmatizer.lemmatize('running', 'v'))\n",
    "print(lemmatizer.lemmatize('ate', 'v'))\n",
    "\n",
    "# lemmatize adjectives\n",
    "print(lemmatizer.lemmatize('saddest', 'a'))\n",
    "print(lemmatizer.lemmatize('fancier', 'a'))\n",
    "\n",
    "# ineffective lemmatization\n",
    "print(lemmatizer.lemmatize('ate', 'n'))\n",
    "print(lemmatizer.lemmatize('fancier', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d0c4d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hour', 'later', ',', 'rumor', 'begin', 'to', 'spread', 'in', 'the', 'close', '-', 'knit', 'community', 'that', 'something', 'have', 'happen', 'to', 'her', 'son', 'in', 'the', 'nearby', 'river', '.', 'Kapessa', '’s', 'old', 'sister', 'hear', 'from', 'friend', 'that', 'Kapessa', ',', 'who', 'could', 'not', 'swim', ',', 'may', 'have', 'jump', 'into', 'the', 'water', ';', 'other', 'allege', 'that', 'he', 'have', 'be', 'push', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "sample_text = \"\"\"Hours later, rumors began to spread in the close-knit community that something \\\n",
    "had happened to her son in the nearby river. Kapessa’s older sister heard from friends that \\\n",
    "Kapessa, who could not swim, may have jumped into the water; others alleged that he had been \\\n",
    "pushed.\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d6562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

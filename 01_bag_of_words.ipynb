{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words\n",
    "Bag-of-Words is a representation of text that describes the occurence of words within a document. It involves two things \n",
    "1. Vocabulary of known words\n",
    "2. A measure of presence of known of words\n",
    "\n",
    "The measure of occurence of known words can be 'counts', 'binary', 'frequency'. 'tf-idf'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \"\"\"The documents of a file loaded \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            doc = file.read()\n",
    "        return doc\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    \"\"\"Tokenization, Removal of numerics, stopwords, punctuations and less frequent tokens\"\"\"\n",
    "    tokens = doc.split(' ')\n",
    "    # remove punctuations\n",
    "    reg_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [reg_punctuation.sub('', w) for w in tokens]\n",
    "    # romove numerics and stop words\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    # remove short tokens\n",
    "    tokens = [token for token in tokens if len(token)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_vocab(directory):\n",
    "    \"\"\"Vocabs is saved to a file\"\"\"\n",
    "    neg_dir = directory + '/neg'\n",
    "    pos_dir = directory + '/pos'\n",
    "    dirs = [pos_dir, neg_dir]\n",
    "    vocab_dict = collections.Counter()\n",
    "    for direc in dirs:\n",
    "        for filename in os.listdir(direc):\n",
    "            file_path = direc + '/' + filename\n",
    "            doc = load_doc(file_path)\n",
    "            doc_tokens = clean_doc(doc)\n",
    "            vocab_dict.update(doc_tokens)\n",
    "    # remove tokens with less than minimum occurence\n",
    "    tokens = [token for token, count in vocab_dict.items() if count>=2]\n",
    "    data = '\\n'.join(tokens)\n",
    "    file = open('vocabulary.txt', 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_doc(directory, vocab):\n",
    "    \"\"\"Preparation of documents for bag-of-words model\"\"\"\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = directory + '/' + filename\n",
    "        doc = load_doc(file_path)\n",
    "        cleaned_doc = clean_doc(doc)\n",
    "        cleaned_doc = [token for token in cleaned_doc if token in vocab]\n",
    "        docs.append(cleaned_doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(docs):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras API is used to converts docs to encoded documents vectors. Keras provides the ```Tokenizer``` class cleans the documents and transforms the document to encoded documents. First ```Tokernier``` object is created and fit on the documents to be transformed. ```Tokenizer``` calls the function ```texts_to_matrix()``` for encoding of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    create_save_vocab('txt_sentoken')\n",
    "    file_path = 'vocabulary.txt'\n",
    "    vocab = load_doc(file_path)\n",
    "    vocab = set(vocab.split())\n",
    "    print('Vocabulary Size: ', len(vocab))\n",
    "    pos_docs = prepare_doc('txt_sentoken/pos', vocab)\n",
    "    neg_docs = prepare_doc('txt_sentoken/neg', vocab)\n",
    "    docs = pos_docs + neg_docs\n",
    "    tokenizer = create_tokenizer(docs)\n",
    "    X = tokenizer.texts_to_matrix(docs, mode='freq')\n",
    "    print('Shape of the encoded Document: ', X.shape)\n",
    "    print('Number of Document: ', X.shape[0])\n",
    "    print('Document vector size: ', X.shape[1])\n",
    "    print(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  26790\n",
      "Shape of the encoded Document:  (2000, 26791)\n",
      "Number of Document:  2000\n",
      "Document vector size:  26791\n",
      "[[0.         0.01312336 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.0147929  0.00591716 ... 0.         0.         0.        ]\n",
      " [0.         0.00490196 0.00980392 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.01522843 0.00507614 ... 0.         0.         0.01015228]\n",
      " [0.         0.01153846 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01123596 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

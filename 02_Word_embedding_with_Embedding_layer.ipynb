{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word Embeddings are a type of word representation in vector form in the predefined vector space where similar words closer in the vector space. Each word is mapped to one vector in the predefined vector space, where the vector is learned using a neural network.\n",
    "\n",
    "### Embedding layer\n",
    "Keras API provides 'Embedding Layer' which can be used for learning word embeddings. But it requires that document be cleaned and prepared. Each word is encoded as one hot vector. The size of vector space is then the size of known vocabulary. The trained weights of the embedding layers makes the embedding matrix, which one hot encoded words are mapped to the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "reviews = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "sentiments = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```one_hot()``` provided by Keras API assigns each word a unique number within the defined range. The define range must be at lease the vocabulary size Here, we have defined the vocabulary size 20. So, each word is assigned a number from 1 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 19], [9, 20], [23, 19], [6, 20], [12], [1], [2, 19], [23, 9], [2, 20], [24, 5, 19, 1]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 30\n",
    "one_hot_encoded_reviews = [one_hot(d, vocab_size) for d in reviews]\n",
    "print(one_hot_encoded_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the inputs should have the same length. The documents with lenth less than the maximum length must be zero padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4 19  0  0  0]\n",
      " [ 9 20  0  0  0]\n",
      " [23 19  0  0  0]\n",
      " [ 6 20  0  0  0]\n",
      " [12  0  0  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 2 19  0  0  0]\n",
      " [23  9  0  0  0]\n",
      " [ 2 20  0  0  0]\n",
      " [24  5 19  1  0]]\n"
     ]
    }
   ],
   "source": [
    "max_length = 5 # maximum lenth of the documents, here the last document has length 4\n",
    "padded_docs = pad_sequences(one_hot_encoded_reviews, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a Sequential model with an embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim of embedding vector\n",
    "vector_dim = 10\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=vector_dim, \n",
    "                           input_length=max_length,\n",
    "                           name='Embedding'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, fit and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "X = padded_docs\n",
    "y = labels\n",
    "\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding (Embedding)        (None, 5, 10)             300       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 351\n",
      "Trainable params: 351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# one hot vector of word 'Well'\n",
    "\n",
    "well_one_hot = np.zeros(shape=(vocab_size,))\n",
    "well_one_hot[4] = 1    # 'well' was assigned the number 4\n",
    "well_one_hot = np.array(well_one_hot)[:,np.newaxis]\n",
    "print(well_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.51123607e-01 -9.53367203e-02 -1.20475568e-01  6.17100894e-02\n",
      "   1.48435742e-01  3.45391303e-01  8.00744444e-02  2.83507884e-01\n",
      "   4.60204817e-02 -9.43058133e-02]\n",
      " [-9.77457389e-02  2.04347163e-01 -3.50886971e-01  2.73626328e-01\n",
      "   3.45430583e-01 -7.51033798e-02 -3.18316191e-01 -3.83961767e-01\n",
      "  -2.25290105e-01 -3.76695216e-01]\n",
      " [-4.39869910e-01  3.62886965e-01 -2.64731318e-01  2.90962964e-01\n",
      "   2.97272891e-01 -3.94604146e-01 -4.18781847e-01 -3.42983127e-01\n",
      "  -2.95747161e-01 -2.93500364e-01]\n",
      " [-1.17079243e-02 -3.39016542e-02  4.88743670e-02 -3.86748537e-02\n",
      "  -2.44597923e-02  2.07471885e-02  7.23155588e-03 -7.88573176e-03\n",
      "  -4.29349020e-03 -1.39094889e-04]\n",
      " [ 3.92505467e-01 -2.70496339e-01  2.16423348e-01 -2.72191942e-01\n",
      "  -2.10554734e-01  3.80384147e-01  3.73877615e-01  2.38095805e-01\n",
      "   2.82703876e-01  2.88644552e-01]\n",
      " [ 1.81708232e-01 -1.57693431e-01 -2.17590198e-01  8.76898244e-02\n",
      "   1.84784934e-01 -1.99227288e-01  1.46499470e-01 -1.67989641e-01\n",
      "   1.64520845e-01 -2.36917213e-01]\n",
      " [ 3.51894170e-01 -2.95314103e-01  2.58267343e-01 -3.13822329e-01\n",
      "  -2.32457295e-01  3.56399626e-01  3.50760192e-01  2.44889870e-01\n",
      "   2.52312988e-01  2.83071131e-01]\n",
      " [ 2.28074454e-02 -2.29238402e-02 -1.75769217e-02  3.75785716e-02\n",
      "  -3.86725776e-02 -4.88928817e-02 -3.79914269e-02  2.79373638e-02\n",
      "  -4.20349948e-02  2.44316496e-02]\n",
      " [ 3.98514308e-02 -4.38856259e-02  6.14061207e-03  1.23732574e-02\n",
      "  -2.65004877e-02 -3.45905051e-02  2.62552761e-02  8.09257105e-03\n",
      "   2.39326693e-02  3.69597189e-02]\n",
      " [ 3.19596529e-01 -2.80845106e-01  2.20625833e-01 -4.00981754e-01\n",
      "  -2.49296531e-01 -3.47961754e-01  3.01535219e-01  2.97205240e-01\n",
      "   3.58351618e-01  1.08698450e-01]\n",
      " [ 1.05228052e-02  2.85913460e-02  2.35535987e-02 -3.91733535e-02\n",
      "   2.54597776e-02  3.97771038e-02 -3.76049876e-02 -4.48812358e-02\n",
      "   1.82592310e-02 -4.82201464e-02]\n",
      " [-4.01574969e-02 -2.43297108e-02 -3.97229567e-02  4.00540493e-02\n",
      "  -3.18790562e-02 -4.96020429e-02  2.94748694e-03 -2.47488376e-02\n",
      "  -9.30577517e-03 -8.89359787e-03]\n",
      " [ 3.92290235e-01 -3.30275208e-01  3.46892953e-01 -2.77497768e-01\n",
      "  -2.78378695e-01  3.67786080e-01  3.47715944e-01  2.77773291e-01\n",
      "   3.28321904e-01  2.89501339e-01]\n",
      " [ 2.73568742e-02  1.06740221e-02  8.24582577e-03 -1.92003734e-02\n",
      "   3.96822356e-02 -3.13980505e-03  2.06075348e-02  7.67629221e-03\n",
      "   1.32986419e-02  1.45068876e-02]\n",
      " [ 9.05216858e-03  1.58850104e-03 -3.68331671e-02 -3.45533863e-02\n",
      "   2.22059749e-02 -2.20943820e-02 -1.46228895e-02  3.53576429e-02\n",
      "   5.93512133e-03 -1.69420838e-02]\n",
      " [ 4.95038070e-02  3.68487835e-03 -3.76538262e-02 -4.04480919e-02\n",
      "  -3.70852463e-02  6.18767738e-03 -2.26625204e-02  1.01601481e-02\n",
      "  -1.21180043e-02 -1.93459913e-03]\n",
      " [-1.17671974e-02 -4.33588885e-02  7.94275850e-03 -4.88821045e-02\n",
      "   2.23846324e-02 -1.57272331e-02  4.74829338e-02  3.03065218e-02\n",
      "  -3.87004986e-02 -2.17952374e-02]\n",
      " [-1.45948306e-02  1.06729046e-02  4.56456207e-02  4.15148474e-02\n",
      "  -3.42365876e-02  3.80726717e-02 -4.50538173e-02 -2.35327482e-02\n",
      "  -1.21050104e-02 -3.98388132e-02]\n",
      " [-4.42796126e-02 -1.93078518e-02  4.47551161e-03 -2.87090894e-02\n",
      "  -3.66623402e-02  7.04776496e-04  2.17130445e-02  9.92547348e-03\n",
      "   4.17542718e-02 -4.13578637e-02]\n",
      " [-3.59992027e-01  3.43115985e-01  2.58754641e-01 -1.04495861e-01\n",
      "  -2.53114551e-01  3.17841798e-01 -3.73127967e-01 -1.57986701e-01\n",
      "  -2.60220796e-01  2.14366719e-01]\n",
      " [-3.03869754e-01  2.16673911e-01  2.09616348e-01 -3.62932645e-02\n",
      "  -2.56035477e-01  1.98954999e-01 -1.94990754e-01  1.85022891e-01\n",
      "  -2.80123383e-01  2.08271131e-01]\n",
      " [ 3.81732024e-02  2.98829414e-02  3.42616327e-02  1.16994157e-02\n",
      "   4.23749536e-03  1.98025741e-02  1.78943910e-02  2.74831988e-02\n",
      "   1.79731287e-02 -5.64312935e-03]\n",
      " [ 1.76454224e-02  1.42266266e-02  1.13129131e-02  3.40606086e-02\n",
      "  -1.87597759e-02  3.08580324e-03 -1.94083583e-02  1.53878443e-02\n",
      "  -1.32063143e-02  3.66349928e-02]\n",
      " [ 2.62503445e-01  1.04952445e-02  3.18969712e-02 -1.61124766e-03\n",
      "   1.86416935e-02  1.46160468e-01  2.73718596e-01  4.57125083e-02\n",
      "   3.60674076e-02 -1.23749208e-02]\n",
      " [-2.98965245e-01  1.77615225e-01 -1.92491233e-01  1.94141105e-01\n",
      "   2.22422674e-01 -2.31913537e-01 -2.70023972e-01 -2.24908501e-01\n",
      "  -2.25928351e-01 -2.25729033e-01]\n",
      " [ 1.80426128e-02 -4.07832861e-02 -1.44099109e-02  2.40308680e-02\n",
      "  -4.55852859e-02 -3.40333730e-02 -8.31414014e-04  2.66668238e-02\n",
      "   1.01830810e-03 -7.32375309e-03]\n",
      " [-4.41199206e-02 -2.76023634e-02  8.79063457e-03  4.21921872e-02\n",
      "   1.67243220e-02 -8.55097920e-03 -1.88387986e-02  4.93309163e-02\n",
      "   3.35894860e-02  1.39396265e-03]\n",
      " [-2.16031205e-02  3.65939029e-02 -8.13229010e-03 -4.62190285e-02\n",
      "   4.89804484e-02 -6.90705702e-03  7.77054951e-03  5.65433502e-03\n",
      "  -2.82584559e-02 -7.22029060e-03]\n",
      " [ 1.45193450e-02  1.04816668e-02 -4.48977463e-02 -1.98545214e-02\n",
      "   4.68575098e-02 -3.62504646e-03  4.92623709e-02  2.38148235e-02\n",
      "   2.25331895e-02  2.53559016e-02]\n",
      " [-8.25818628e-03 -9.42289829e-03 -7.50708580e-03  2.29506604e-02\n",
      "   1.86953656e-02  1.12682097e-02 -4.13345695e-02  2.09043361e-02\n",
      "   8.73066112e-03  8.29490274e-03]]\n",
      "(30, 10)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = model.get_layer('Embedding').get_weights()[0]\n",
    "print(embedding_matrix)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39250547 -0.27049634  0.21642335 -0.27219194 -0.21055473  0.38038415\n",
      "   0.37387761  0.23809581  0.28270388  0.28864455]]\n"
     ]
    }
   ],
   "source": [
    "# Embedding vector of 'well'\n",
    "\n",
    "well_emb_vector = embedding_matrix.T.dot(well_one_hot)\n",
    "print(well_emb_vector.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:  https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
